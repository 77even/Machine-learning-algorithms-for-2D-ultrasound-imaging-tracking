# -*- coding: utf-8 -*-
"""2D Needle Tracking in ultrasound.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CzJQixCKRbnSLMWBk6OdbjxsCA8QFy8C

# **Introduction**

***Tasks:*** Design an algorithm which should be developed in Python using PyTorch. The aim is to review recent advances in supervised learning methods and to design an algorithm within this context to estimate the elevation position of the needle in real time. The algorithm will attempt to identify features in the FOH waveform to predict this estimation.

***Objectives:*** Using ultrasound images, a recording of the true position of the needle is used to achieve a prediction of the needle position using a supervised learning machine learning approach. Performed in a 2D context.

***Data:*** The dataset is derived from a catalogue of data shared by Christian Baker as part of a previous experiment, each catalogue being a repeat of the experiment in which the needle tip was transformed into a set of positions within the imaging plane (in a rectangular grid). The catalogues are very large (> 10 GB). They contain HDF5 files - one HDF5 file per location.

We have only selected the data in the first directory to carry out our project, namely "0-degrees-test-3_(raw_data_fig8_9)". This file contains at least 18 repeated measurements. Each measurement consists of a "FOH frame" and a "US frame":


*   A FOH frame is a 2D array. Each column of the array is the waveform received by the hydrophone during a single ultrasound transmission from the US imaging probe. This means that each row of the array is a different time step and that each column of the array has a different angle along the probe face. Both temporal and angular resolution are included. Only one FOH frame is needed to determine the position of the tip of the needle.

*   The US frame is the ultrasound image generated by the imaging system.


***Task Process:***



*   The algorithm design and pipeline implementation was undertaken by Jiaqi Li, which included extracting the target data from the data shared by Dr Baker, processing the data, designing the algorithm pipeline, training the network and validating it, testing the model and making predictions on needle locations, and analysing and dynamically visualising the predictions.

*   The preparation of the project is in the hands of Wenzhuo Sun, who is responsible for reading the literature, analysing the data and contributing ideas to the algorithm, while the optimisation part of the project will be her responsibility to optimise the pipeline and achieve better results.

# **Algorithmic implementation**

---

### **Check the data for 20.0_-5.0_0.0.h5 as an example**

---
"""

import os
import sys
import h5py
from google.colab import drive

# # Mount your drive to colab
drive.mount('/content/drive')

# Change to the path to the folder with the input files (YOU MIGHT NEED TO CHANGE IT)
# make sure you upload all data and .py files to this directory on your drive
root_dir = '/content/drive/My Drive/Group_research'
os.chdir(root_dir)
os.listdir()

# Open the HDF5 file
with h5py.File('/content/drive/My Drive/Group_research/DATA/0-degrees-test-3_(raw_data_fig8_9)/20.0_-5.0_0.0.h5', 'r') as f:
    # List the names of all datasets in the file
    print(list(f.keys()))

!apt-get install -y h5utils

!h5ls -r "/content/drive/My Drive/Group_research/DATA/0-degrees-test-3_(raw_data_fig8_9)/20.0_-5.0_0.0.h5"

import xono
from pathlib import Path

from matplotlib.pyplot import figure, imshow, plot, show, xlabel, ylabel

from xono.helpers import create_time_axis, get_true_position, get_waveform_angles_in_degrees
from xono.serialisation.xono_archive_reader import xono_archive_reader_factory

# Load data path
FILE_PATH = Path("/content/drive/My Drive/Group_research/DATA/0-degrees-test-3_(raw_data_fig8_9)/20.0_-5.0_0.0.h5")

# Read the HDF5 files
if __name__ == "__main__":

    reader = xono_archive_reader_factory(FILE_PATH)

    # get true position in millimeters
    true_needle_position = get_true_position(FILE_PATH)
    print(f"True position: lateral: {true_needle_position.x} mm, axial: {true_needle_position.y} mm")


    # get number of US frames ("imaging" frames)
    print(f"There are {reader.us_frame_count} FOH frames.")

    # get a frame by its index
    us_frame = reader.us_by_index(9)

    # get image from US frame and plot it
    us_image = us_frame.read_pixels()
    figure()
    imshow(us_image)

    show()

# Load data path
FILE_PATH = Path("/content/drive/My Drive/Group_research/DATA/0-degrees-test-3_(raw_data_fig8_9)/20.0_-5.0_0.0.h5")

# Read the HDF5 files
if __name__ == "__main__":

    reader = xono_archive_reader_factory(FILE_PATH)

    us_frame = reader.us_by_index(9)

    x = us_frame.resolution_header.x_mm_per_pixel
    y = us_frame.resolution_header.y_mm_per_pixel

    print(x)
    print(y)

"""## **Extraction of the ultrasound image data and the corresponding true position of the needle from the HDF5 file**

---


"""

# # Mount your drive to colab
from google.colab import drive
drive.mount('/content/drive')

import os
from pathlib import Path
import numpy as np
from matplotlib.pyplot import figure, imshow, plot, show, xlabel, ylabel

# make sure you upload all data and .py files to this directory on your drive
root_dir = '/content/drive/My Drive/Group_research'
os.chdir(root_dir)
os.listdir()

import xono
from xono.helpers import create_time_axis, get_true_position, get_waveform_angles_in_degrees
from xono.serialisation.xono_archive_reader import xono_archive_reader_factory

import os
from pathlib import Path

# Set directory path
DIRECTORY_PATH = Path("/content/drive/My Drive/Group_research/DATA/0-degrees-test-3_(raw_data_fig8_9)/")

if __name__ == "__main__":
  # Loop through all HDF5 files in the directory
  for file_name in os.listdir(DIRECTORY_PATH):
      if file_name.endswith(".h5"):
          # Load data path
          file_path = DIRECTORY_PATH / file_name

          try:
              # Read the HDF5 file
              reader = xono_archive_reader_factory(file_path)

              # get true position in millimeters
              true_needle_position = get_true_position(file_path)
              print(f"True position: lateral: {true_needle_position.x} mm, axial: {true_needle_position.y} mm")

              # get number of US frames ("imaging" frames)
              print(f"There are {reader.us_frame_count} US frames.")

              # get a frame by its index
              if reader.us_frame_count > 0:
                  us_frame_index = min(1, reader.us_frame_count - 1)  # Ensure index is within range
                  us_frame = reader.us_by_index(us_frame_index)

                  # get image from US frame and plot it
                  us_image = us_frame.read_pixels()
              else:
                  print("No US frames in this file.")
          except (OSError, KeyError, IndexError) as e:
              print(f"Error reading file {file_path}: {e}")
              if isinstance(e, IndexError):
                print(f"This error may be caused by an invalid HDF5 file format.")

"""### **Processing**"""

import os
from pathlib import Path
import numpy as np
from matplotlib.pyplot import imshow, show

def process_files(directory_path):
    true_positions = []
    us_images = []

    for filename in os.listdir(directory_path):
        if filename.endswith(".h5"):
            file_path = os.path.join(directory_path, filename)

            try:

              # Read the HDF5 files
              reader = xono_archive_reader_factory(Path(file_path))

              us_frame = reader.us_by_index(0)
              us_image = us_frame.read_pixels()

              # Get true position in millimeters
              true_needle_position = get_true_position(Path(file_path))

              # Append the true position and ultrasound image to the lists
              true_positions.append([true_needle_position.x, true_needle_position.y])
              us_images.append(us_image)

            except (OSError, KeyError, IndexError) as e:
              print(f"Error reading file {file_path}: {e}")
              if isinstance(e, IndexError):
                print(f"This error may be caused by an invalid HDF5 file format.")

    # Convert lists to NumPy arrays
    true_positions_array = np.array(true_positions)
    us_images_array = np.array(us_images)

    return true_positions_array, us_images_array

def save_data(true_positions, us_images, output_dir):
    os.makedirs(output_dir, exist_ok=True)
    np.save(os.path.join(output_dir, "true_positions_Final.npy"), true_positions)
    np.save(os.path.join(output_dir, "us_images_final.npy"), us_images)

if __name__ == "__main__":
    DIRECTORY_PATH = "/content/drive/My Drive/Group_research/DATA/0-degrees-test-3_(raw_data_fig8_9)/"
    OUTPUT_DIR = "/content/drive/My Drive/Group_research/DATA/"

    true_positions, us_images = process_files(DIRECTORY_PATH)
    save_data(true_positions, us_images, OUTPUT_DIR)
    print("Data saved successfully.")

"""### **Check the results after processing**"""

# Specify the path to the directory containing the saved arrays
OUTPUT_DIR = "/content/drive/My Drive/Group_research/DATA/"

# Load the true positions array
true_positions = np.load(OUTPUT_DIR + "true_positions_Final.npy")

# Load the ultrasound images array
us_images = np.load(OUTPUT_DIR + "us_images_final.npy")

print(f"Shape of the ultrasound dataset: {us_images.shape}")
print(f"Shape of the true positions dataset: {true_positions.shape}")

"""### **Convert the position to match the images**"""

import numpy as np
from matplotlib.pyplot import figure, imshow, plot, show

# # Mount your drive to colab
from google.colab import drive
drive.mount('/content/drive')

# Specify the path to the directory containing the saved arrays
OUTPUT_DIR = "/content/drive/My Drive/Group_research/DATA/"

# Load the true positions array
true_positions = np.load(OUTPUT_DIR + "true_positions_Final.npy")

# Load the ultrasound images array
us_images = np.load(OUTPUT_DIR + "us_images_final.npy")

# us_images = np.expand_dims(us_images, axis=1)

figure()
imshow(us_images[0])
show()

print("First real position: ({:.2f}, {:.2f}) mm".format(true_positions[0][0], true_positions[0][1]))

"""### **Check that the position of the conversion matches the original image**

---


"""

# Probe position in the image
probe_position_image = np.array([730, 298])

# Corresponding real-world coordinates of the probe position (in millimeters)
probe_position_real = true_positions[0] # assuming the first true position corresponds to the probe position

# Calculate the scaling factors for the x and y directions
scale_x = probe_position_real[0] / probe_position_image[0]
scale_y = probe_position_real[1] / probe_position_image[1]


# Step 4: Convert the true positions to image coordinates
image_positions = np.zeros_like(true_positions)
image_positions[:, 0] = true_positions[:, 0] / scale_x
image_positions[:, 1] = true_positions[:, 1] / scale_y

# Step 5: Plot the true positions on the ultrasound images
for i in range(len(us_images)):
    plt.imshow(us_images[i])
    plt.scatter(image_positions[i, 0], image_positions[i, 1], c='r', marker='x')
    plt.title(f"Ultrasound image {i+1}")
    plt.show()

"""### **Convert and save**

---


"""

import numpy as np
import matplotlib.pyplot as plt
from PIL import Image

# Probe position in the image (in pixels)730 298
probe_position_image = np.array([730, 298])

# Corresponding real-world coordinates of the probe position (in millimeters)
probe_position_real = true_positions[0] # assuming the first true position corresponds to the probe position

# Calculate the scaling factors for the x and y directions
scale_x = probe_position_real[0] / probe_position_image[0]
scale_y = probe_position_real[1] / probe_position_image[1]


# Convert the true positions to image coordinates
image_positions = np.zeros_like(true_positions)
image_positions[:, 0] = true_positions[:, 0] / scale_x
image_positions[:, 1] = true_positions[:, 1] / scale_y

# Save the converted real positions as a NumPy array
np.save('/content/drive/My Drive/Group_research/converted_real_positions.npy', image_positions)

"""### **Images with location markers**

---


"""

import numpy as np
import cv2

# Load the data path
image_file = "/content/drive/My Drive/Group_research/DATA/us_images_final.npy"
position_file = "/content/drive/My Drive/Group_research/DATA/converted_real_positions.npy"

images = np.load(image_file)
positions = np.load(position_file)

# Create the function to add the needle representation to the ultrasound images
def add_needle_to_image(image, position, size=10, color=(0, 255, 255), thickness=2):
    modified_image = image.copy()
    int_position = tuple(map(int, position))  # Convert position values to integers

    start_x, end_x = int_position[0] - size, int_position[0] + size
    start_y, end_y = int_position[1] - size, int_position[1] + size

    cv2.line(modified_image, (start_x, int_position[1]), (end_x, int_position[1]), color, thickness)
    cv2.line(modified_image, (int_position[0], start_y), (int_position[0], end_y), color, thickness)

    return modified_image

# Iterate through the original images and their corresponding positions, apply the function to each pair, and create a new set of modified images
modified_images = []

for image, position in zip(images, positions):
    modified_image = add_needle_to_image(image, position)
    modified_images.append(modified_image)

modified_images = np.array(modified_images)

modified_image_file = "/content/drive/My Drive/Group_research/us_images_with_needle.npy"
np.save(modified_image_file, modified_images)

# Load the images with positions
image_file = "/content/drive/My Drive/Group_research/us_images_with_needle.npy"

images = np.load(image_file)

figure()
imshow(images[0])
show()

"""## **Correct extraction of images and coordinates**

---


"""

# # Mount your drive to colab
from google.colab import drive
drive.mount('/content/drive')

import os
from pathlib import Path
import numpy as np
from matplotlib.pyplot import figure, imshow, plot, show, xlabel, ylabel

# make sure you upload all data and .py files to this directory on your drive
root_dir = '/content/drive/My Drive/Group_research'
os.chdir(root_dir)
os.listdir()

import xono
from xono.helpers import create_time_axis, get_true_position, get_waveform_angles_in_degrees
from xono.serialisation.xono_archive_reader import xono_archive_reader_factory

import os
from pathlib import Path
import numpy as np
from matplotlib.pyplot import imshow, show


def process_files(directory_path):
    true_positions = []
    us_images = []

    for filename in os.listdir(directory_path):
        if filename.endswith(".h5"):
            file_path = os.path.join(directory_path, filename)

            try:

              # Read the HDF5 files
              reader = xono_archive_reader_factory(Path(file_path))

              us_frame = reader.us_by_index(0)
              us_image = us_frame.read_pixels()




              us_frame.resolution_header.x_mm_per_pixel
              us_frame.resolution_header.y_mm_per_pixel
              us_frame.resolution_header.origin_x_coord_in_mm
              us_frame.resolution_header.origin_y_coord_in_mm


              # Get true position in millimeters
              true_needle_position = get_true_position(Path(file_path))

              x_needle = true_needle_position.x
              y_needle = true_needle_position.y

              x_true = (x_needle+us_frame.resolution_header.origin_x_coord_in_mm)/us_frame.resolution_header.x_mm_per_pixel
              y_true = (y_needle+us_frame.resolution_header.origin_y_coord_in_mm)/us_frame.resolution_header.y_mm_per_pixel


              # Append the true position and ultrasound image to the lists
              true_positions.append([x_true, y_true])
              us_images.append(us_image)

            except (OSError, KeyError, IndexError) as e:
              print(f"Error reading file {file_path}: {e}")
              if isinstance(e, IndexError):
                print(f"This error may be caused by an invalid HDF5 file format.")

    # Convert lists to NumPy arrays
    true_positions_array = np.array(true_positions)
    us_images_array = np.array(us_images)

    return true_positions_array, us_images_array

def save_data(true_positions, us_images, output_dir):
    os.makedirs(output_dir, exist_ok=True)
    np.save(os.path.join(output_dir, "True_positions.npy"), true_positions)
    np.save(os.path.join(output_dir, "Us_images.npy"), us_images)

if __name__ == "__main__":
    DIRECTORY_PATH = "/content/drive/My Drive/Group_research/DATA/0-degrees-test-3_(raw_data_fig8_9)/"
    OUTPUT_DIR = "/content/drive/My Drive/Group_research/"

    true_positions, us_images = process_files(DIRECTORY_PATH)
    save_data(true_positions, us_images, OUTPUT_DIR)
    print("Data saved successfully.")

# Load the data path
image_file = "/content/drive/My Drive/Group_research/Us_images.npy"
position_file = "/content/drive/My Drive/Group_research/True_positions.npy"

# Visualisation
# Load the images and positions data
images = np.load(image_file)
positions = np.load(position_file)

figure()
imshow(images[0])
show()

# Display position on an image
figure()
imshow(images[0])
plot(positions[0][0],positions[0][1],c='r', marker='x')
show()

"""## **Pipeline implement**

---

### **Visualisation**

---
"""

import numpy as np
from matplotlib.pyplot import figure, imshow, plot, show
import matplotlib.pyplot as plt
from google.colab import drive

# Mount your drive to colab
drive.mount('/content/drive')

# Load the data path
image_file = "/content/drive/My Drive/Group_research/Us_images.npy"
position_file = "/content/drive/My Drive/Group_research/True_positions.npy"

# # Load the data path
# image_file = "/content/drive/My Drive/Group_research/DATA/us_images_final.npy"
# position_file = "/content/drive/My Drive/Group_research/DATA/converted_real_positions.npy"

# Visualisation
# Load the images and positions data
images = np.load(image_file)
positions = np.load(position_file)

figure()
imshow(images[0])
show()

# Display position on an image
figure()
imshow(images[0])
plot(positions[0][0],positions[0][1],c='r', marker='x')
show()

print("First real position: ({:.2f}, {:.2f})".format(positions[0][0], positions[0][1]))

"""### **Division(*Please ignore, use the divided file*)**

---


"""

from sklearn.model_selection import train_test_split

# Load the images and positions data
images = np.load(image_file)
positions = np.load(position_file)

# Splitting into 70% train, 15% validation, and 15% test
X_temp, X_test, y_temp, y_test = train_test_split(images, positions, test_size=0.15, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.1765, random_state=42)

# Save the train, validation, and test sets as .npy files
np.save('/content/drive/My Drive/Group_research/NEW/train_images.npy', X_train)
np.save('/content/drive/My Drive/Group_research/NEW/train_positions.npy', y_train)
np.save('/content/drive/My Drive/Group_research/NEW/val_images.npy', X_val)
np.save('/content/drive/My Drive/Group_research/NEW/val_positions.npy', y_val)
np.save('/content/drive/My Drive/Group_research/NEW/test_images.npy', X_test)
np.save('/content/drive/My Drive/Group_research/NEW/test_positions.npy', y_test)

"""### **Loading**

---


"""

import os
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
import cv2
from matplotlib.pyplot import figure, imshow, plot, show
import matplotlib.pyplot as plt
from google.colab import drive

# Mount your drive to colab
drive.mount('/content/drive')

# Load the data path
train_image_file = "/content/drive/My Drive/Group_research/NEW/train_images.npy"
train_position_file = "/content/drive/My Drive/Group_research/NEW/train_positions.npy"

val_image_file = "/content/drive/My Drive/Group_research/NEW/val_images.npy"
val_position_file = "/content/drive/My Drive/Group_research/NEW/val_positions.npy"

test_image_file = "/content/drive/My Drive/Group_research/NEW/test_images.npy"
test_position_file = "/content/drive/My Drive/Group_research/NEW/test_positions.npy"

# Check the train data
images = np.load(train_image_file)
positions = np.load(train_position_file)

print(images.shape)
print(positions.shape)

figure()
imshow(images[0])
plot(positions[0][0],positions[0][1], c='r', marker='x')
show()

# Check the valid data
images = np.load(val_image_file)
positions = np.load(val_position_file)

print(images.shape)
print(positions.shape)

figure()
imshow(images[0])
plot(positions[0][0],positions[0][1], c='r', marker='x')
show()

# Check the test data
images = np.load(test_image_file)
positions = np.load(test_position_file)

print(images.shape)
print(positions.shape)

figure()
imshow(images[0])
plot(positions[0][0],positions[0][1], c='r', marker='x')
show()

"""### **Datasets and data loaders**

---


"""

# If use the Resnet 50, use this dataset class
# # Define the custom dataset
# class UltrasoundDataset(Dataset):
#     def __init__(self, image_file, position_file, transform=None):
#         self.images = np.load(image_file)
#         self.positions = np.load(position_file)
#         self.transform = transform

#     def __len__(self):
#         return len(self.images)

#     def __getitem__(self, idx):
#         image = self.images[idx]
#         position = self.positions[idx]

#         # Convert grayscale image to RGB by duplicating the single channel
#         image = np.stack((image,) * 3, axis=-1)

#         if self.transform:
#             image = self.transform(image)

#         return image, position

# import random
# from torchvision.transforms import functional as F
# from PIL import Image

# def data_transform_train(image, position):

#     # # Convert numpy array to PIL Image
#     # image = F.to_pil_image(image)
#     image = Image.fromarray(image)

#     # Apply horizontal flip with probability 0.5
#     if random.random() < 0.5:
#         image = transforms.functional.hflip(image)
#         position[0] = 1 - position[0]  # Assuming position values are normalized

#     # Apply vertical flip with probability 0.5
#     if random.random() < 0.5:
#         image = transforms.functional.vflip(image)
#         position[1] = 1 - position[1]  # Assuming position values are normalized

#     # Apply other transformations to the image
#     image = transforms.Compose([
#         transforms.ToPILImage(),
#         transforms.ToTensor(),
#         transforms.Normalize(mean=[0.5], std=[0.5]),
#         transforms.RandomRotation(degrees=15),
#         transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),
#     ])(image)

#     return image, position

# from torchvision.transforms import functional as F

# class UltrasoundDataset(Dataset):
#     def __init__(self, image_file, position_file, transform=None, position_transform=None):
#         self.images = np.load(image_file)
#         self.positions = np.load(position_file)
#         self.transform = transform
#         self.position_transform = position_transform

#     def __len__(self):
#         return len(self.images)

#     def __getitem__(self, idx):
#         image = self.images[idx]
#         position = self.positions[idx]

#         if self.transform:
#             image = self.transform(image)

#         if self.position_transform:
#             position = self.position_transform(position, image)

#         return image, position

# class PositionFlip:
#     def __init__(self, horizontal_flip_prob=0.5, vertical_flip_prob=0.5):
#         self.horizontal_flip_prob = horizontal_flip_prob
#         self.vertical_flip_prob = vertical_flip_prob

#     def __call__(self, position, image):
#         # Apply horizontal flip
#         if np.random.random() < self.horizontal_flip_prob:
#             position[0] = image.shape[1] - position[0]

#         # Apply vertical flip
#         if np.random.random() < self.vertical_flip_prob:
#             position[1] = image.shape[0] - position[1]

#         return position

# Define the custom dataset
class UltrasoundDataset(Dataset):
    def __init__(self, image_file, position_file, transform=None):
        self.images = np.load(image_file)
        self.positions = np.load(position_file)
        self.transform = transform

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        image = self.images[idx]
        position = self.positions[idx]

        if self.transform:
            image = self.transform(image)

        return image, position

# Apply the same flip transformation to the position as to the image
import random
class UltrasoundDataset(Dataset):
    def __init__(self, image_file, position_file, transform=None):
        self.images = np.load(image_file)
        self.positions = np.load(position_file)
        self.transform = transform

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        image = self.images[idx]
        position = self.positions[idx]

        if self.transform:
            image = self.transform(image)
            # Get the horizontal and vertical flipping status of the image
            h_flipped = isinstance(self.transform.transforms[-3], transforms.RandomHorizontalFlip)
            v_flipped = isinstance(self.transform.transforms[-4], transforms.RandomVerticalFlip)
            # Flip the position data accordingly
            if h_flipped:
                position[0] = -position[0]
            if v_flipped:
                position[1] = -position[1]


        return image, position

data_transform_train = transforms.Compose([
    transforms.ToPILImage(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomVerticalFlip(p=0.5),
    transforms.RandomRotation(degrees=15),
    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),

])

data_transform = transforms.Compose([
    transforms.ToPILImage(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5])
])

# Create the dataset and data loaders
train_dataset = UltrasoundDataset(train_image_file, train_position_file, transform=data_transform_train)
train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)

val_dataset = UltrasoundDataset(val_image_file, val_position_file, transform=data_transform)
val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4)

test_dataset = UltrasoundDataset(test_image_file, test_position_file, transform=data_transform)
test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=4)

import numpy as np
import matplotlib.pyplot as plt

# Get a sample from the training dataset
sample_idx = 0  # Choose any valid index
sample_image, sample_position = train_dataset[sample_idx]

# Convert the transformed image back to a NumPy array for visualization
transformed_image_np = sample_image.numpy().transpose(1, 2, 0)
transformed_image_np = (transformed_image_np * 0.5) + 0.5  # Reverse normalization

# Visualize the transformed image and position data
plt.imshow(transformed_image_np)
plt.scatter(sample_position[0], sample_position[1], c="r", marker="x")
plt.show()

import numpy as np
import matplotlib.pyplot as plt

# Get a sample from the training dataset
sample_idx = 1  # Choose any valid index
sample_image, sample_position = train_dataset[sample_idx]

# Convert the transformed image back to a NumPy array for visualization
transformed_image_np = sample_image.numpy().transpose(1, 2, 0)
transformed_image_np = (transformed_image_np * 0.5) + 0.5  # Reverse normalization

# Visualize the transformed image and position data
plt.imshow(transformed_image_np)
plt.scatter(sample_position[0], sample_position[1], c="r", marker="x")
plt.show()

import numpy as np
import matplotlib.pyplot as plt

# Get a sample from the training dataset
sample_idx = 2  # Choose any valid index
sample_image, sample_position = train_dataset[sample_idx]

# Convert the transformed image back to a NumPy array for visualization
transformed_image_np = sample_image.numpy().transpose(1, 2, 0)
transformed_image_np = (transformed_image_np * 0.5) + 0.5  # Reverse normalization

# Visualize the transformed image and position data
plt.imshow(transformed_image_np)
plt.scatter(sample_position[0], sample_position[1], c="r", marker="x")
plt.show()

import numpy as np
import matplotlib.pyplot as plt

# Get a sample from the training dataset
sample_idx = 3  # Choose any valid index
sample_image, sample_position = train_dataset[sample_idx]

# Convert the transformed image back to a NumPy array for visualization
transformed_image_np = sample_image.numpy().transpose(1, 2, 0)
transformed_image_np = (transformed_image_np * 0.5) + 0.5  # Reverse normalization

# Visualize the transformed image and position data
plt.imshow(transformed_image_np)
plt.scatter(sample_position[0], sample_position[1], c="r", marker="x")
plt.show()

import numpy as np
import matplotlib.pyplot as plt

# Get a sample from the training dataset
sample_idx = 4  # Choose any valid index
sample_image, sample_position = train_dataset[sample_idx]

# Convert the transformed image back to a NumPy array for visualization
transformed_image_np = sample_image.numpy().transpose(1, 2, 0)
transformed_image_np = (transformed_image_np * 0.5) + 0.5  # Reverse normalization

# Visualize the transformed image and position data
plt.imshow(transformed_image_np)
plt.scatter(sample_position[0], sample_position[1], c="r", marker="x")
plt.show()

"""### **Models, loss functions and optimisers**

---


"""

class NeedlePositionPredictor(nn.Module):
    def __init__(self):
        super(NeedlePositionPredictor, self).__init__()

        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)
        self.relu1 = nn.ReLU()
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)

        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)
        self.relu2 = nn.ReLU()
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)

        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)
        self.relu3 = nn.ReLU()
        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)

        # self.fc1 = nn.Linear(64 * 28 * 28, 512)
        self.fc1 = nn.Linear(64 * 100 * 180, 512) #Use the original size
        self.relu4 = nn.ReLU()

        self.fc2 = nn.Linear(512, 128)
        self.relu5 = nn.ReLU()

        self.fc3 = nn.Linear(128, 2)

    def forward(self, x):
        x = self.conv1(x)
        x = self.relu1(x)
        x = self.pool1(x)

        x = self.conv2(x)
        x = self.relu2(x)
        x = self.pool2(x)

        x = self.conv3(x)
        x = self.relu3(x)
        x = self.pool3(x)

        x = x.view(x.size(0), -1)

        x = self.fc1(x)
        x = self.relu4(x)

        x = self.fc2(x)
        x = self.relu5(x)

        x = self.fc3(x)

        return x

# # Instantiate the model, loss function, and optimizer
# # Set the device to GPU
# device = torch.device("cuda:0")

# model = NeedlePositionPredictor().to(device)
# loss_fn = nn.MSELoss()
# optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-3)

# Instantiate the model, loss function, and optimizer
# Set the device to GPU
device = torch.device("cuda:0")

model = NeedlePositionPredictor().to(device)
loss_fn = nn.L1Loss()
optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-3)

"""Use pre-model ResNet50"""

# import torchvision.models as models
# from torchvision.models.resnet import ResNet50_Weights

# # Set the device to GPU
# device = torch.device("cuda:0")

# model = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1).to(device)


# num_ftrs = model.fc.in_features
# model.fc = nn.Linear(num_ftrs, 2)
# model = model.to(device)

# loss_fn = nn.MSELoss()
# optimizer = optim.Adam(model.parameters(), lr=1e-4)

"""### **Train and Validation**

---


"""

# train the model CNN use MSE loss function
num_epochs = 300
best_val_loss = float('inf')

for epoch in range(num_epochs):
    # Training loop
    for i, (images, positions) in enumerate(train_dataloader):
        images = images.to(device)
        positions = positions.to(device).float()  # Convert to float
        optimizer.zero_grad()
        outputs = model(images)
        loss = loss_fn(outputs, positions)
        loss.backward()
        optimizer.step()

    # Validation loop
    val_loss = 0
    with torch.no_grad():
        for images, positions in val_dataloader:
            images = images.to(device)
            positions = positions.to(device).float()  # Convert to float
            outputs = model(images)
            loss = loss_fn(outputs, positions)
            val_loss += loss.item()

    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item()}, Val Loss: {val_loss/len(val_dataloader)}')
    # Save the model if it has the best validation loss so far
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model.state_dict(), 'needle_position_predictor.pth')
        print("Model saved.")

# train the model CNN use MAE loss function
num_epochs = 300
best_val_loss = float('inf')

train_losses_history = []
val_losses_history = []

for epoch in range(num_epochs):
    # Training loop
    for i, (images, positions) in enumerate(train_dataloader):
        images = images.to(device)
        positions = positions.to(device).float()  # Convert to float
        optimizer.zero_grad()
        outputs = model(images)
        loss = loss_fn(outputs, positions)
        loss.backward()
        optimizer.step()

    # Validation loop
    val_loss = 0
    with torch.no_grad():
        for images, positions in val_dataloader:
            images = images.to(device)
            positions = positions.to(device).float()  # Convert to float
            outputs = model(images)
            loss = loss_fn(outputs, positions)
            val_loss += loss.item()

    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item()}, Val Loss: {val_loss/len(val_dataloader)}')

    train_losses_history.append(loss.item())
    val_losses_history.append(val_loss/len(val_dataloader))

    # Save the model if it has the best validation loss so far
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model.state_dict(), 'needle_position_predictor2.pth')
        print("Model saved.")

# train the model CNN use MAE loss function
num_epochs = 300
best_val_loss = float('inf')

train_losses_history = []
val_losses_history = []

for epoch in range(num_epochs):
    # Training loop
    for i, (images, positions) in enumerate(train_dataloader):
        images = images.to(device)
        positions = positions.to(device).float()  # Convert to float
        optimizer.zero_grad()
        outputs = model(images)
        loss = loss_fn(outputs, positions)
        loss.backward()
        optimizer.step()

    # Validation loop
    val_loss = 0
    with torch.no_grad():
        for images, positions in val_dataloader:
            images = images.to(device)
            positions = positions.to(device).float()  # Convert to float
            outputs = model(images)
            loss = loss_fn(outputs, positions)
            val_loss += loss.item()

    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item()}, Val Loss: {val_loss/len(val_dataloader)}')

    train_losses_history.append(loss.item())
    val_losses_history.append(val_loss/len(val_dataloader))

    # Save the model if it has the best validation loss so far
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model.state_dict(), '/content/drive/My Drive/Group_research/needle_position_predictor_final.pth')
        print("Model saved.")

# Plot the losses
plt.plot(train_losses_history, label='Training Loss')
plt.plot(val_losses_history, label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

"""### **Test the pre-trained model(MSE loss)**

---


"""

# Load the model for evaluation
model.load_state_dict(torch.load('needle_position_predictor.pth'))
model.eval()

# Evaluate the model on test data
with torch.no_grad():
    for i, (images, positions) in enumerate(test_dataloader):
        images = images.to(device)
        positions = positions.to(device).float()  # Convert to float
        outputs = model(images)
        print(f"Predicted: {outputs.cpu()}, Actual: {positions.cpu()}")

import torch.nn.functional as F
import math

# Initialize error variables
mae = 0
mse = 0

# Evaluate the model on test data
with torch.no_grad():
    for i, (images, positions) in enumerate(test_dataloader):
        images = images.to(device)
        positions = positions.to(device).float()  # Convert to float
        outputs = model(images)

        # Calculate MAE and MSE
        mae += F.l1_loss(outputs, positions).item()
        mse += F.mse_loss(outputs, positions).item()

        print(f"Predicted: {outputs.cpu()}, Actual: {positions.cpu()}")

# Calculate mean error values
mae /= len(test_dataloader)
mse /= len(test_dataloader)
mse = math.sqrt(mse)

print(f"Mean Absolute Error: {mae:.4f}")
print(f"Mean Squared Error: {mse:.4f}")

"""The first image of the test set is used as an example to show the position of the real needle

---


"""

# Access first image and its corresponding position
image, position = test_dataset[0]

# Check image shape and data type
print(image.shape)
print(image.dtype)

print(position.shape)
print(position.dtype)

figure()
imshow(image.numpy().squeeze())
plot(position[0],position[1], c='r', marker='x')
show()

print("First real position: ({:.2f}, {:.2f})".format(positions[0][0], positions[0][1]))

"""For the first image of the test set as an example, used to show the predicted needle position

---


"""

# Access first image and its corresponding position
image, position = test_dataset[0]

# Check image shape and data type
print(image.shape)
print(image.dtype)

outputs = outputs.cpu().numpy().squeeze()

print(outputs.shape)
print(outputs.dtype)

figure()
imshow(image.numpy().squeeze())
plot(outputs[0],outputs[1], c='r', marker='x')
show()

print("First pred position: ({:.2f}, {:.2f})".format(outputs[0], outputs[1]))

"""### **Test the pre-trained model(MAE loss)**

---


"""

# Load the model for evaluation
model.load_state_dict(torch.load('needle_position_predictor2.pth'))
model.eval()

# Load the model for evaluation
model.load_state_dict(torch.load('/content/drive/My Drive/Group_research/needle_position_predictor_final.pth'))
model.eval()

# Evaluate the model on test data
with torch.no_grad():
    for i, (images, positions) in enumerate(test_dataloader):
        images = images.to(device)
        positions = positions.to(device).float()  # Convert to float
        outputs = model(images)
        print(f"Predicted: {outputs.cpu()}, Actual: {positions.cpu()}")

"""Calculate the mean absolute error (MAE) and mean squared error (MSE) of the model's predictions for the test dataset

---


"""

import torch.nn.functional as F
import math

# Initialize error variables
mae = 0
mse = 0

mm_per_pixel = 0.4  # The mm per pixel value

# Evaluate the model on test data
with torch.no_grad():
    for i, (images, positions) in enumerate(test_dataloader):
        images = images.to(device)
        positions = positions.to(device).float()  # Convert to float
        outputs = model(images)

        # Convert the outputs and positions to millimeters
        outputs_mm = outputs * mm_per_pixel
        positions_mm = positions * mm_per_pixel

        # Calculate MAE and MSE
        mae += F.l1_loss(outputs_mm, positions_mm).item()
        mse += F.mse_loss(outputs_mm, positions_mm).item()

        print(f"Predicted: {outputs_mm.cpu()}, Actual: {positions_mm.cpu()}")

# Calculate mean error values
mae /= len(test_dataloader)
mse /= len(test_dataloader)
rmse = math.sqrt(mse)

print(f"Mean Absolute Error: {mae:.4f}")
print(f"Root Mean Squared Error: {rmse:.4f}")

import torch.nn.functional as F
import math

# Initialize error variables
mae = 0
mse = 0

# Evaluate the model on test data
with torch.no_grad():
    for i, (images, positions) in enumerate(test_dataloader):
        images = images.to(device)
        positions = positions.to(device).float()  # Convert to float
        outputs = model(images)

        # Calculate MAE and MSE
        mae += F.l1_loss(outputs, positions).item()
        mse += F.mse_loss(outputs, positions).item()

        print(f"Predicted: {outputs.cpu()}, Actual: {positions.cpu()}")

# Calculate mean error values
mae /= len(test_dataloader)
mse /= len(test_dataloader)
mse = math.sqrt(mse)

print(f"Mean Absolute Error: {mae:.4f}")
print(f"Mean Squared Error: {mse:.4f}")

"""Calculate the average distance

---


"""

import torch
import numpy as np

# Evaluate the model on test data
with torch.no_grad():
    avg_dist = 0
    num_samples = 0
    for i, (images, positions) in enumerate(test_dataloader):
        images = images.to(device)
        positions = positions.to(device).float()
        outputs = model(images)
        # Convert the predicted and actual positions to numpy arrays
        predicted = outputs.cpu().numpy() * 0.4
        actual = positions.cpu().numpy() * 0.4
        # Compute the pairwise distances between the predicted and actual positions
        dists = np.sqrt(np.sum((predicted - actual)**2, axis=1))
        # Compute the average distance and update the running average
        batch_size = len(images)
        batch_avg_dist = np.mean(dists)
        avg_dist = (num_samples * avg_dist + batch_size * batch_avg_dist) / (num_samples + batch_size)
        num_samples += batch_size
        print(f"Pred Position {i+1}: Average distance: {batch_avg_dist}")

print(f"Total: Average distance: {avg_dist}")

!pip install seaborn

import torch
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Evaluate the model on test data
with torch.no_grad():
    avg_dist = 0
    num_samples = 0
    all_dists = []  # List to store the distances for all samples
    for i, (images, positions) in enumerate(test_dataloader):
        images = images.to(device)
        positions = positions.to(device).float()
        outputs = model(images)
        # Convert the predicted and actual positions to numpy arrays
        predicted = outputs.cpu().numpy() * 0.4
        actual = positions.cpu().numpy() * 0.4
        # Compute the pairwise distances between the predicted and actual positions
        dists = np.sqrt(np.sum((predicted - actual) ** 2, axis=1))
        all_dists.extend(dists)  # Add the distances to the list
        # Compute the average distance and update the running average
        batch_size = len(images)
        batch_avg_dist = np.mean(dists)
        avg_dist = (num_samples * avg_dist + batch_size * batch_avg_dist) / (num_samples + batch_size)
        num_samples += batch_size
        print(f"Pred Position {i + 1}: Average distance: {batch_avg_dist}")

    print(f"Total: Average distance: {avg_dist}")

# Visualize the results as a heatmap
distance_matrix = np.array(all_dists).reshape(-1, 1)
plt.figure(figsize=(60, 10))
sns.heatmap(distance_matrix.T, annot=True, fmt='.2f', cmap='coolwarm', cbar=True, yticklabels=['Distance'])
plt.xlabel('Sample Index')
plt.ylabel('Distance')
plt.title('Heatmap of Distances Between Predicted and Actual Positions')
plt.show()

import torch
import numpy as np
import matplotlib.pyplot as plt

# Evaluate the model on test data
with torch.no_grad():
    avg_dist = 0
    num_samples = 0
    all_dists = []  # List to store the distances for all samples
    for i, (images, positions) in enumerate(test_dataloader):
        images = images.to(device)
        positions = positions.to(device).float()
        outputs = model(images)
        # Convert the predicted and actual positions to numpy arrays
        predicted = outputs.cpu().numpy() * 0.4
        actual = positions.cpu().numpy() * 0.4
        # Compute the pairwise distances between the predicted and actual positions
        dists = np.sqrt(np.sum((predicted - actual) ** 2, axis=1))
        all_dists.extend(dists)  # Add the distances to the list
        # Compute the average distance and update the running average
        batch_size = len(images)
        batch_avg_dist = np.mean(dists)
        avg_dist = (num_samples * avg_dist + batch_size * batch_avg_dist) / (num_samples + batch_size)
        num_samples += batch_size
        print(f"Pred Position {i + 1}: Average distance: {batch_avg_dist}")

    print(f"Total: Average distance: {avg_dist}")

# Visualize the results as a scatter plot
plt.figure(figsize=(6, 4))
plt.scatter(range(1, len(all_dists) + 1), all_dists)
plt.xlabel('Test images')
plt.ylabel('Distance')
plt.title('Scatter Plot of Distances Between Predicted and Actual Positions')
plt.show()

import torch
import numpy as np
import matplotlib.pyplot as plt

# Evaluate the model on test data
with torch.no_grad():
    avg_dist = 0
    num_samples = 0
    all_dists = []  # List to store the distances for all samples
    for i, (images, positions) in enumerate(test_dataloader):
        images = images.to(device)
        positions = positions.to(device).float()
        outputs = model(images)
        # Convert the predicted and actual positions to numpy arrays
        predicted = outputs.cpu().numpy() * 0.4
        actual = positions.cpu().numpy() * 0.4
        # Compute the pairwise distances between the predicted and actual positions
        dists = np.sqrt(np.sum((predicted - actual) ** 2, axis=1))
        all_dists.extend(dists)  # Add the distances to the list
        # Compute the average distance and update the running average
        batch_size = len(images)
        batch_avg_dist = np.mean(dists)
        avg_dist = (num_samples * avg_dist + batch_size * batch_avg_dist) / (num_samples + batch_size)
        num_samples += batch_size
        print(f"Pred Position {i + 1}: Average distance: {batch_avg_dist}")

    print(f"Total: Average distance: {avg_dist}")

# Visualize the results as a scatter plot
plt.figure(figsize=(6, 4))
plt.scatter(range(1, len(all_dists) + 1), all_dists)
plt.axhline(avg_dist, color='red', linestyle='--', label=f'Average Distance: {avg_dist:.2f}')  # Add the average distance line
plt.xlabel('Test images')
plt.ylabel('Distance')
plt.title('Scatter Plot of Distances Between Predicted and Actual Positions')
plt.legend()  # Add a legend to show the average distance label
plt.show()

# Set a distance threshold value (in the same units as the positions)
threshold = 10

# Initialize the number of correct predictions
num_correct = 0

# ... (keep the existing code)
# Evaluate the model on test data
with torch.no_grad():
    avg_dist = 0
    num_samples = 0
    all_dists = []  # List to store the distances for all samples

    for i, (images, positions) in enumerate(test_dataloader):
        # ... (keep the existing code)
        images = images.to(device)
        positions = positions.to(device).float()
        outputs = model(images)
        # Convert the predicted and actual positions to numpy arrays
        predicted = outputs.cpu().numpy() * 0.4
        actual = positions.cpu().numpy() * 0.4
        # Compute the pairwise distances between the predicted and actual positions
        dists = np.sqrt(np.sum((predicted - actual) ** 2, axis=1))
        all_dists.extend(dists)  # Add the distances to the list
        # Compute the average distance and update the running average
        batch_size = len(images)
        batch_avg_dist = np.mean(dists)
        avg_dist = (num_samples * avg_dist + batch_size * batch_avg_dist) / (num_samples + batch_size)
        num_samples += batch_size
        print(f"Pred Position {i + 1}: Average distance: {batch_avg_dist}")

        # Compare distances with the threshold and count correct predictions
        correct_predictions = np.sum(dists <= threshold)
        num_correct += correct_predictions

# Calculate the accuracy
accuracy = num_correct / num_samples
print(f"Accuracy: {accuracy * 100:.2f}%")

import torch
import numpy as np

total_distance_error = 0
total_samples = 0

with torch.no_grad():
    for i, (images, positions) in enumerate(test_dataloader):
        images = images.to(device)
        positions = positions.to(device).float()  # Convert to float
        outputs = model(images)

        # Calculate Euclidean distance between predicted and actual positions
        distance = torch.sqrt(torch.sum((outputs - positions)**2, dim=1))
        total_distance_error += torch.sum(distance).item()
        total_samples += positions.size(0)

        print(f"Predicted: {outputs.cpu()}, Actual: {positions.cpu()}")

# Calculate the average distance error
average_distance_error = total_distance_error / total_samples
print(f"Average Distance Error: {average_distance_error}")

import torch
import numpy as np

mm_per_pixel = 0.4
total_distance_error_mm = 0
total_samples = 0

with torch.no_grad():
    for i, (images, positions) in enumerate(test_dataloader):
        images = images.to(device)
        positions = positions.to(device).float()  # Convert to float
        outputs = model(images)

        # Convert positions to millimeters
        positions_mm = positions * mm_per_pixel
        outputs_mm = outputs * mm_per_pixel

        # Calculate Euclidean distance between predicted and actual positions in millimeters
        distance_mm = torch.sqrt(torch.sum((outputs_mm - positions_mm)**2, dim=1))
        total_distance_error_mm += torch.sum(distance_mm).item()
        total_samples += positions.size(0)

        print(f"Predicted (mm): {outputs_mm.cpu()}, Actual (mm): {positions_mm.cpu()}")

# Calculate the average distance error in millimeters
average_distance_error_mm = total_distance_error_mm / total_samples
print(f"Average Distance Error (mm): {average_distance_error_mm}")

"""Calculate the accuracy by comparing the difference between the predicted and actual positions

---


"""

# Set the model to evaluation mode
model.eval()

# Set a threshold for accepting the difference between predictions and ground truth
threshold = 10

# Initialize variables to count correct predictions and total predictions
correct_predictions = 0
total_predictions = 0

# Disable gradient computation
with torch.no_grad():
    # Loop through the test data using the test_dataloader
    for i, (images, positions) in enumerate(test_dataloader):
        # Move images and positions to the GPU (if available)
        images = images.to(device)
        positions = positions.to(device).float()  # Convert to float

        # Pass the images through the model to get the predicted positions
        outputs = model(images)

        # Calculate the difference between the predicted positions and actual positions
        diff = torch.abs(outputs - positions)

        # Count the predictions that fall within the threshold
        correct_predictions += torch.sum(diff <= threshold).item()
        total_predictions += images.size(0)

# Calculate the accuracy
accuracy = correct_predictions / total_predictions * 100

# Print the accuracy
print(f"Accuracy: {accuracy:.2f}%")

"""### **Dynamic display for the prediction of model use MSE loss**

---


"""

import time
import matplotlib.pyplot as plt

# Evaluate the model on test data
with torch.no_grad():
    for i, (images, positions) in enumerate(test_dataloader):
        images = images.to(device)
        positions = positions.to(device).float()  # Convert to float
        outputs = model(images)

        print(f"Predicted: {outputs.cpu()}, True: {positions.cpu()}")

        # Convert the image from a PyTorch tensor to a NumPy array and remove the first dimension
        image = images[0].cpu().numpy().squeeze()

        # Move positions and outputs to CPU, convert to NumPy array and remove batch dimension
        actual_position = positions[0].cpu().numpy().squeeze()
        predicted_position = outputs[0].cpu().numpy().squeeze()

        # Plot the image, real needle position, and predicted position
        figure()
        imshow(image)
        plot(actual_position[0], actual_position[1], c='r', marker='x', label='True')
        plot(predicted_position[0], predicted_position[1], c='purple', marker='x', label='Predicted')

        plt.legend()
        plt.title(f"Image {i+1}")
        plt.show()

        # Add a delay between plots for better visualization
        time.sleep(1)

# Initialize empty lists to store real and predicted positions
real_positions = []
predicted_positions = []

# Evaluate the model on test data and collect real and predicted positions
with torch.no_grad():
    for i, (images, positions) in enumerate(test_dataloader):
        images = images.to(device)
        positions = positions.to(device).float()  # Convert to float
        outputs = model(images)

        real_positions.append(positions.cpu().numpy())
        predicted_positions.append(outputs.cpu().numpy())

# Convert lists to NumPy arrays
real_positions = np.array(real_positions).squeeze()
predicted_positions = np.array(predicted_positions).squeeze()

# Create a scatter plot of real and predicted positions
plt.scatter(real_positions[:, 0], real_positions[:, 1], c='r', marker='x', label='Real')
plt.scatter(predicted_positions[:, 0], predicted_positions[:, 1], c='purple', marker='o', label='Predicted')

plt.legend()
plt.title("Track Map: Real vs. Predicted Needle Positions")
plt.xlabel("X Position")
plt.ylabel("Y Position")
plt.show()

"""Create an animated scatter plot that shows the real and predicted needle positions frame by frame"""

import numpy as np
import matplotlib.pyplot as plt
from IPython.display import clear_output

# Evaluate the model on test data and collect real and predicted positions
real_positions = []
predicted_positions = []

with torch.no_grad():
    for i, (images, positions) in enumerate(test_dataloader):
        images = images.to(device)
        positions = positions.to(device).float()  # Convert to float
        outputs = model(images)

        real_positions.append(positions.cpu().numpy())
        predicted_positions.append(outputs.cpu().numpy())

# Convert lists to NumPy arrays
real_positions = np.array(real_positions).squeeze()
predicted_positions = np.array(predicted_positions).squeeze()

# Display the frames dynamically
for i in range(len(real_positions)):
    plt.scatter(real_positions[:i + 1, 0], real_positions[:i + 1, 1], c='r', marker='x', label='Real' if i == 0 else '')
    plt.scatter(predicted_positions[:i + 1, 0], predicted_positions[:i + 1, 1], c='purple', marker='o', label='Predicted' if i == 0 else '')

    # plt.xlim(0, 1440)
    # plt.ylim(0, 800)
    plt.legend()
    plt.title("Track Map: Real vs. Predicted Needle Positions")
    plt.xlabel("X Position")
    plt.ylabel("Y Position")

    clear_output(wait=True)
    plt.pause(0.2)
    if i != len(real_positions) - 1:
        plt.clf()

plt.show()

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation

# Evaluate the model on test data and collect real and predicted positions
real_positions = []
predicted_positions = []

with torch.no_grad():
    for i, (images, positions) in enumerate(test_dataloader):
        images = images.to(device)
        positions = positions.to(device).float()
        outputs = model(images)

        real_positions.append(positions.cpu().numpy())
        predicted_positions.append(outputs.cpu().numpy())

# Convert lists to NumPy arrays
real_positions = np.array(real_positions).squeeze()
predicted_positions = np.array(predicted_positions).squeeze()

# Set up the figure for the animation
fig, ax = plt.subplots()

def update_plot(frame):
    ax.clear()
    ax.scatter(real_positions[:frame + 1, 0], real_positions[:frame + 1, 1], c='r', marker='x', label='Real')
    ax.scatter(predicted_positions[:frame + 1, 0], predicted_positions[:frame + 1, 1], c='purple', marker='o', label='Predicted')

    ax.set_xlim(0, 1440)
    ax.set_ylim(0, 800)
    ax.legend()
    ax.set_title("Track Map: Real vs. Predicted Needle Positions")
    ax.set_xlabel("X Position")
    ax.set_ylabel("Y Position")

ani = FuncAnimation(fig, update_plot, frames=len(real_positions), interval=200, repeat=False)
ani.save('/content/drive/My Drive/Group_research/track_map_animation.mp4', writer='ffmpeg', dpi=100)
# ani.save('/content/drive/My Drive/Group_research/track_map_animation.gif', writer='imagemagick', dpi=100)
plt.close(fig)

"""### **Dynamic display for the prediction of model use MAE loss**

---


"""

# Load the model for evaluation
model.load_state_dict(torch.load('/content/drive/My Drive/Group_research/needle_position_predictor_final.pth'))
model.eval()

# Set the model to evaluation mode
model.eval()

# Set a threshold for accepting the difference between predictions and ground truth
threshold = 8

# Initialize variables to count correct predictions and total predictions
correct_predictions = 0
total_predictions = 0

# Disable gradient computation
with torch.no_grad():
    # Loop through the test data using the test_dataloader
    for i, (images, positions) in enumerate(test_dataloader):
        # Move images and positions to the GPU (if available)
        images = images.to(device)
        positions = positions.to(device).float()  # Convert to float

        # Pass the images through the model to get the predicted positions
        outputs = model(images)

        # Calculate the difference between the predicted positions and actual positions
        diff = torch.abs(outputs - positions)

        # Count the predictions that fall within the threshold
        correct_predictions += torch.sum(diff <= threshold).item()
        total_predictions += images.size(0)

# Calculate the accuracy
accuracy = correct_predictions / total_predictions * 100

# Print the accuracy
print(f"Accuracy: {accuracy:.2f}%")

import time
import matplotlib.pyplot as plt

# Evaluate the model on test data
with torch.no_grad():
    for i, (images, positions) in enumerate(test_dataloader):
        images = images.to(device)
        positions = positions.to(device).float()  # Convert to float
        outputs = model(images)

        print(f"Predicted: {outputs.cpu()}, True: {positions.cpu()}")

        # Convert the image from a PyTorch tensor to a NumPy array and remove the first dimension
        image = images[0].cpu().numpy().squeeze()

        # Move positions and outputs to CPU, convert to NumPy array and remove batch dimension
        actual_position = positions[0].cpu().numpy().squeeze()
        predicted_position = outputs[0].cpu().numpy().squeeze()

        # Plot the image, real needle position, and predicted position
        figure()
        imshow(image)
        plot(actual_position[0], actual_position[1], c='r', marker='x', label='True')
        plot(predicted_position[0], predicted_position[1], c='purple', marker='x', label='Predicted')

        plt.legend()
        plt.title(f"Image {i+1}")
        plt.show()

        # Add a delay between plots for better visualization
        time.sleep(1)

import time
import matplotlib.pyplot as plt
from PIL import Image
import imageio

# Initialize a list to store the file paths of the saved images
image_files = []

# Evaluate the model on test data
with torch.no_grad():
    for i, (images, positions) in enumerate(test_dataloader):
        images = images.to(device)
        positions = positions.to(device).float()  # Convert to float
        outputs = model(images)

        print(f"Predicted: {outputs.cpu()}, True: {positions.cpu()}")

        # Convert the image from a PyTorch tensor to a NumPy array and remove the first dimension
        image = images[0].cpu().numpy().squeeze()

        # Move positions and outputs to CPU, convert to NumPy array and remove batch dimension
        actual_position = positions[0].cpu().numpy().squeeze()
        predicted_position = outputs[0].cpu().numpy().squeeze()

        # Plot the image, real needle position, and predicted position
        plt.figure(figsize=(10, 8))  # Set the width and height of the figure

        plt.imshow(image)
        plt.plot(actual_position[0], actual_position[1], c='r', marker='x', label='True')
        plt.plot(predicted_position[0], predicted_position[1], c='purple', marker='x', label='Predicted')

        plt.legend()
        plt.title(f"Image {i+1}")

        # Generate a file path for the image and save it
        file_path = f"image_{i+1}.png"
        plt.savefig(file_path)

        # Add the file path to the list
        image_files.append(file_path)

        # Close the current figure to release memory
        plt.close()

        # Add a delay between plots for better visualization
        time.sleep(1)

# Create a GIF from the saved images
gif_file_path = "/content/drive/My Drive/Group_research/images.gif"
images = []
for file_path in image_files:
    image = Image.open(file_path)
    images.append(image)

images[0].save(gif_file_path, save_all=True, append_images=images[1:], loop=0, duration=500)

# # Create an MP4 file from the saved images
# mp4_file_path = "/content/drive/My Drive/Group_research/images.mp4"
# with imageio.get_writer(mp4_file_path, format="mp4", mode="I", fps=1) as writer:
#     for file_path in image_files:
#         image = imageio.imread(file_path)
#         writer.append_data(image)

# Initialize empty lists to store real and predicted positions
real_positions = []
predicted_positions = []

# Evaluate the model on test data and collect real and predicted positions
with torch.no_grad():
    for i, (images, positions) in enumerate(test_dataloader):
        images = images.to(device)
        positions = positions.to(device).float()  # Convert to float
        outputs = model(images)

        real_positions.append(positions.cpu().numpy())
        predicted_positions.append(outputs.cpu().numpy())

# Convert lists to NumPy arrays
real_positions = np.array(real_positions).squeeze()
predicted_positions = np.array(predicted_positions).squeeze()

# Create a scatter plot of real and predicted positions
plt.scatter(real_positions[:, 0], real_positions[:, 1], c='r', marker='x', label='Real')
plt.scatter(predicted_positions[:, 0], predicted_positions[:, 1], c='purple', marker='x', label='Predicted')

plt.legend()
plt.title("Track Map: Real vs. Predicted Needle Positions")
plt.xlabel("X Position")
plt.ylabel("Y Position")
plt.show()

# Initialize empty lists to store real and predicted positions
real_positions = []
predicted_positions = []

# Evaluate the model on test data and collect real and predicted positions
with torch.no_grad():
    for i, (images, positions) in enumerate(test_dataloader):
        images = images.to(device)
        positions = positions.to(device).float()  # Convert to float
        outputs = model(images)

        real_positions.append(positions.cpu().numpy())
        predicted_positions.append(outputs.cpu().numpy())

# Convert lists to NumPy arrays
real_positions = np.array(real_positions).squeeze()
predicted_positions = np.array(predicted_positions).squeeze()

# Create a scatter plot of real and predicted positions
plt.scatter(real_positions[:, 0], real_positions[:, 1], c='r', marker='x', label='Real')
plt.scatter(predicted_positions[:, 0], predicted_positions[:, 1], c='purple', marker='x', label='Predicted')

# Set the limits of the x and y axes
plt.xlim(0, 1440)
plt.ylim(0, 800)
plt.legend()
plt.title("Track Map: Real vs. Predicted Needle Positions")
plt.xlabel("X Position")
plt.ylabel("Y Position")
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation

# Evaluate the model on test data and collect real and predicted positions
real_positions = []
predicted_positions = []

with torch.no_grad():
    for i, (images, positions) in enumerate(test_dataloader):
        images = images.to(device)
        positions = positions.to(device).float()
        outputs = model(images)

        real_positions.append(positions.cpu().numpy())
        predicted_positions.append(outputs.cpu().numpy())

# Convert lists to NumPy arrays
real_positions = np.array(real_positions).squeeze()
predicted_positions = np.array(predicted_positions).squeeze()

# Set up the figure for the animation
fig, ax = plt.subplots()

def update_plot(frame):
    ax.clear()
    ax.scatter(real_positions[:frame + 1, 0], real_positions[:frame + 1, 1], c='r', marker='x', label='Real')
    ax.scatter(predicted_positions[:frame + 1, 0], predicted_positions[:frame + 1, 1], c='purple', marker='o', label='Predicted')

    ax.set_xlim(0, 1440)
    ax.set_ylim(0, 800)
    ax.legend()
    ax.set_title("Track Map: Real vs. Predicted Needle Positions")
    ax.set_xlabel("X Position")
    ax.set_ylabel("Y Position")

ani = FuncAnimation(fig, update_plot, frames=len(real_positions), interval=200, repeat=False)
ani.save('/content/drive/My Drive/Group_research/track_map_animation_MAEloss.mp4', writer='ffmpeg', dpi=100)
# ani.save('/content/drive/My Drive/Group_research/track_map_animation_MAEloss.gif', writer='imagemagick', dpi=100)
plt.close(fig)

"""### **Conclusion**

---

**Description of the algorithm implementation:**

Design a machine learning model for predicting the position of a needle in an ultrasound image. The code loads image and position data from files, splits the data into training, validation, and testing sets, defines a convolutional neural network model, trains the model using mean absolute error loss, evaluates the model on the test set, and creates visualizations of the predicted needle positions compared to the real positions.

The specific steps involved in this implement are:



*   Load the image and position data from files


*   Split the data into training, validation, and testing sets


*   Define a convolutional neural network model called NeedlePositionPredictor(CNN-based model architecture)


*   Train the model using mean absolute error loss and an Adam optimizer


*   Evaluate the model on the test set using mean absolute error and mean squared error metrics


*   Create visualizations of the predicted needle positions compared to the real positions


*   Create a scatter plot of real and predicted needle positions

*   Create an animation comparing the real position of the needle on the track with the predicted position

**Analysis of results:**

In 300 epochs, the final training loss was 22.314716339111328 and the validation loss was 20.465124893188477, which indicates that there is some overfitting of the model in training, whereby there may be too little data causing the model to learn too well without better generalisation. It is also possible that the model architecture is not complex enough and needs to be optimised. Consider applying regularisation techniques to prevent overfitting, or adjusting appropriate hyperparameters to help the model generalise better. For the data augmentation part, I used some data augmentation. More complex data augmentation methods could be considered.

Based on the validation loss and evaluation metrics, the model appears to perform reasonably well on the test data. 15.2045 mean absolute error and 24.0093 mean square error indicate that the model makes predictions that are relatively close to the true values. However, optimisation is still needed to achieve better optimisation.

Based on the prediction results, I set a threshold with a value of 10 below which the prediction results were treated as largely successful. A comparison with the real position of the needle on the test set of 70 images showed an accuracy of 95.71%, which basically achieved the purpose of using ultrasound images for needle prediction
"""